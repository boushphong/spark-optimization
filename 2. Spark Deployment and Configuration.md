# Spark App Execution

![image](https://user-images.githubusercontent.com/59940078/181920667-070a3e50-27f3-4447-8efc-40b60b19de4c.png)

# The Anatomy of a Cluster

![image](https://user-images.githubusercontent.com/59940078/181920699-8ca17243-1fa5-4aa1-a9df-4cb6fa522803.png)

1) **Cluster Mode**
- The Spark driver is launched on a worker node.
- The Cluster Manager is responsible for Spark processes.

![image](https://user-images.githubusercontent.com/59940078/181920834-2c59a307-90ac-4cb5-aa40-b97634881f1f.png)

2) **Client Mode**
- The Spark driver is on the client machine.
- The Client is responsible for the Spark processes and state management.

![image](https://user-images.githubusercontent.com/59940078/181920844-6078ca26-e687-4721-88d2-99102526c7f4.png)

3) **Local Mode**
- The entire application runs on the same machine.
 
# **Configuring Spark Resources**

## Method 1. Config on Spark Application's Creation

```
val spark = SparkSession.builder()
      .appName("Test Deploy App")
      // method 1
      .config("spark.executor.memory", "1g")
      .getOrCreate()
```

## Method 2. Config while Spark Application is running

```
spark.conf.set("spark.executor.memory", "1g") // warning - not all configurations available
```

## Method 3. Config in the command line arguments
```
spark-submit ... --conf spark.executor.memory 1g

You can also use dedicated command line arguments for certain configurations:
    --master = spark.master
    --executor-memory = spark.executor.memory
    --driver-memory = spark.driver.memory

    and many more.
```
